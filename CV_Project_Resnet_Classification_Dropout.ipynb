{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import MySQLdb\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.transforms as transforms\n",
    "import torch.utils.data.sampler as smp\n",
    "#from tqdm import tqdm\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from io import BytesIO\n",
    "import IPython.display\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "\n",
    "import dataset\n",
    "import dataLoader\n",
    "import configure as cf\n",
    "import plot_utils as utils\n",
    "import train_function as train\n",
    "import resnet as modified_resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def class2id(name):\n",
    "    if(name == 'food'):\n",
    "        return 0\n",
    "    elif (name == 'inside'):\n",
    "        return 1\n",
    "    elif (name == 'outside'):\n",
    "        return 2\n",
    "    elif (name == 'drink'):\n",
    "        return 3\n",
    "    elif (name == 'menu'):\n",
    "        return 4\n",
    "    else:\n",
    "        return 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class YelpDataSet(torch.utils.data.Dataset):\n",
    "    def __init__(self, photo_dir, category, transform=None):\n",
    "        self.photo_dir = photo_dir + '/photos'\n",
    "        self.category = category\n",
    "        self.transform = transform\n",
    "\n",
    "        conn = MySQLdb.connect(host=cf.mysql_ip, user=cf.mysql_user, passwd=cf.mysql_pwd,\n",
    "                               db=cf.mysql_db_name, charset=\"utf8\")\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        # Get all photo id with the label = category\n",
    "        self.photo_id = []\n",
    "        print category\n",
    "        if(category == 'all'):\n",
    "            cursor.execute('select id, label from photo')\n",
    "        else:\n",
    "            cursor.execute('select id, label from photo where label=\\'' + category + '\\'')\n",
    "        for row in cursor.fetchall():\n",
    "            tem_dic = dict()\n",
    "            tem_dic['id'] = row[0]\n",
    "            tem_dic['label'] = row[1]\n",
    "            self.photo_id.append(tem_dic)\n",
    "\n",
    "            # self.photo_id: list of dict\n",
    "            #    dict keys: id\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.photo_id)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_address = self.photo_dir + '/' + self.photo_id[idx]['id'] + '.jpg'\n",
    "        image = Image.open(img_address).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        # dic = {'image': image, 'stars': float(self.photo_id[idx]['stars'])}\n",
    "        label = self.photo_id[idx]['label']\n",
    "        return image, class2id(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_train_valid_loader(photo_dir,\n",
    "                               category,\n",
    "                               batch_size=1,\n",
    "                               random_seed=32,\n",
    "                               transform=None,\n",
    "                               valid_size=0.1,\n",
    "                               set_num = -1,\n",
    "                               shuffle=True,\n",
    "                               num_workers=4,\n",
    "                               pin_memory=False):\n",
    "\n",
    "        #error_msg = \"[!] valid_size should be in the range [0, 1].\"\n",
    "        #assert ((valid_size >= 0) and (valid_size <= 1)), error_msg\n",
    "\n",
    "        # load the dataset\n",
    "        yelpDataset = YelpDataSet(photo_dir, category, transform)\n",
    "        num_train = len(yelpDataset)\n",
    "        indices = list(range(num_train))\n",
    "        if set_num == -1:\n",
    "            set_sum = num_train\n",
    "        else:\n",
    "            set_sum = set_num\n",
    "        split = int(np.floor(valid_size * set_sum))\n",
    "\n",
    "        if shuffle:\n",
    "            np.random.seed(random_seed)\n",
    "            np.random.shuffle(indices)\n",
    "\n",
    "        train_idx, valid_idx = indices[split:set_sum], indices[:split]\n",
    "\n",
    "        train_sampler = smp.SubsetRandomSampler(train_idx)\n",
    "        valid_sampler = smp.SubsetRandomSampler(valid_idx)\n",
    "\n",
    "        train_loader = torch.utils.data.DataLoader(yelpDataset,\n",
    "                                                   batch_size=batch_size, sampler=train_sampler,\n",
    "                                                   num_workers=num_workers, pin_memory=pin_memory)\n",
    "\n",
    "        valid_loader = torch.utils.data.DataLoader(yelpDataset,\n",
    "                                                   batch_size=batch_size, sampler=valid_sampler,\n",
    "                                                   num_workers=num_workers, pin_memory=pin_memory)\n",
    "\n",
    "        return train_loader, valid_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all\n"
     ]
    }
   ],
   "source": [
    "#define learningRate\n",
    "learningRate = 1e-3 \n",
    "\n",
    "# Definition of our network.\n",
    "network = modified_resnet.resnet50(pretrained = True)\n",
    "network.fc = nn.Linear(512*4, 5)\n",
    "\n",
    "#Definition of our loss.\n",
    "#The MSELoss function \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Definition of optimization strategy.\n",
    "optimizer = optim.SGD(network.parameters(), lr = learningRate)\n",
    "\n",
    "imgTransform = transforms.Compose([transforms.Scale(256),\n",
    "                                   transforms.CenterCrop(224),\n",
    "                                   transforms.ToTensor(),\n",
    "                                   transforms.Normalize((0.4914, 0.4822, 0.4465), \n",
    "                                                        (0.2023, 0.1994, 0.2010))])\n",
    "\n",
    "trainLoader, valLoader = get_train_valid_loader(cf.photo_url,'all', 50, 32,imgTransform,0.1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_model(network, criterion, optimizer, trainLoader, valLoader, batch_size = 50, n_epochs = 10, use_gpu = False):\n",
    "    train_accuracy = []\n",
    "    train_loss = []\n",
    "    val_accuracy = []\n",
    "    val_loss = []\n",
    "\n",
    "    if use_gpu:\n",
    "        network = network.cuda()\n",
    "        criterion = criterion.cuda()\n",
    "        \n",
    "    # Training loop.\n",
    "    for epoch in range(0, n_epochs):\n",
    "        correct = 0.0\n",
    "        cum_loss = 0.0\n",
    "        counter = 0\n",
    "\n",
    "        # Make a pass over the training data.\n",
    "        t = tqdm(trainLoader, desc = 'Training epoch %d' % epoch)\n",
    "        network.train()  # This is important to call before training!\n",
    "        for (i, (inputs, labels)) in enumerate(t):\n",
    "\n",
    "            # Wrap inputs, and targets into torch.autograd.Variable types.\n",
    "            inputs = Variable(inputs)\n",
    "            labels = Variable(labels)\n",
    "            if inputs.size(0) < batch_size or labels.size(0) < batch_size: continue\n",
    "            \n",
    "            if use_gpu:\n",
    "                inputs = inputs.cuda()\n",
    "                labels = labels.cuda()\n",
    "\n",
    "            # Forward pass:\n",
    "            outputs = network(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward pass:\n",
    "            optimizer.zero_grad()\n",
    "            # Loss is a variable, and calling backward on a Variable will\n",
    "            # compute all the gradients that lead to that Variable taking on its\n",
    "            # current value.\n",
    "            loss.backward() \n",
    "\n",
    "            # Weight and bias updates.\n",
    "            optimizer.step()\n",
    "\n",
    "            # logging information.\n",
    "            cum_loss += loss.data[0]\n",
    "            max_scores, max_labels = outputs.data.max(1)\n",
    "            correct += (max_labels == labels.data).sum()\n",
    "            counter += inputs.size(0)\n",
    "            t.set_postfix(loss = cum_loss / (1 + i), accuracy = 100 * correct / counter)\n",
    "        train_accuracy.append(100 * correct / counter)\n",
    "        train_loss.append(cum_loss / (1 + i))\n",
    "\n",
    "        # Make a pass over the validation data.\n",
    "        correct = 0.0\n",
    "        cum_loss = 0.0\n",
    "        counter = 0\n",
    "        t = tqdm(valLoader, desc = 'Validation epoch %d' % epoch)\n",
    "        network.eval()  # This is important to call before evaluating!\n",
    "        for (i, (inputs, labels)) in enumerate(t):\n",
    "\n",
    "            # Wrap inputs, and targets into torch.autograd.Variable types.\n",
    "            inputs = Variable(inputs)\n",
    "            labels = Variable(labels)\n",
    "            if inputs.size(0) < batch_size or labels.size(0) < batch_size: continue\n",
    "            \n",
    "            if use_gpu:\n",
    "                inputs = inputs.cuda()\n",
    "                labels = labels.cuda()\n",
    "\n",
    "            # Forward pass:\n",
    "            outputs = network(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # logging information.\n",
    "            cum_loss += loss.data[0]\n",
    "            max_scores, max_labels = outputs.data.max(1)\n",
    "            correct += (max_labels == labels.data).sum()\n",
    "            counter += inputs.size(0)\n",
    "            t.set_postfix(loss = cum_loss / (1 + i), accuracy = 100 * correct / counter)\n",
    "        val_accuracy.append(100 * correct / counter)\n",
    "        val_loss.append(cum_loss / (1 + i))\n",
    "    return [train_accuracy, val_accuracy, train_loss, val_loss]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f1a4421212042d3bc4ac87a6ac1127e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-19:\n",
      "KeyboardInterrupt\n",
      "Process Process-20:\n",
      "Process Process-18:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/razqtest1/anaconda2/lib/python2.7/multiprocessing/process.py\", line 267, in _bootstrap\n",
      "  File \"/home/razqtest1/anaconda2/lib/python2.7/multiprocessing/process.py\", line 267, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/razqtest1/anaconda2/lib/python2.7/multiprocessing/process.py\", line 114, in run\n",
      "Process Process-17:\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "    self.run()\n",
      "  File \"/home/razqtest1/anaconda2/lib/python2.7/multiprocessing/process.py\", line 267, in _bootstrap\n",
      "  File \"/home/razqtest1/anaconda2/lib/python2.7/multiprocessing/process.py\", line 267, in _bootstrap\n",
      "    self.run()\n",
      "    self.run()\n",
      "  File \"/home/razqtest1/anaconda2/lib/python2.7/site-packages/torch/utils/data/dataloader.py\", line 34, in _worker_loop\n",
      "  File \"/home/razqtest1/anaconda2/lib/python2.7/multiprocessing/process.py\", line 114, in run\n",
      "  File \"/home/razqtest1/anaconda2/lib/python2.7/multiprocessing/process.py\", line 114, in run\n",
      "  File \"/home/razqtest1/anaconda2/lib/python2.7/multiprocessing/process.py\", line 114, in run\n",
      "    r = index_queue.get()\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/razqtest1/anaconda2/lib/python2.7/site-packages/torch/utils/data/dataloader.py\", line 40, in _worker_loop\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/razqtest1/anaconda2/lib/python2.7/multiprocessing/queues.py\", line 376, in get\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "    return recv()\n",
      "  File \"<ipython-input-14-e05f0342cee9>\", line 32, in __getitem__\n",
      "  File \"/home/razqtest1/anaconda2/lib/python2.7/site-packages/torch/utils/data/dataloader.py\", line 34, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/home/razqtest1/anaconda2/lib/python2.7/multiprocessing/queues.py\", line 374, in get\n",
      "    racquire()\n",
      "  File \"/home/razqtest1/anaconda2/lib/python2.7/site-packages/torch/utils/data/dataloader.py\", line 40, in _worker_loop\n",
      "  File \"/home/razqtest1/anaconda2/lib/python2.7/site-packages/torch/multiprocessing/queue.py\", line 21, in recv\n",
      "KeyboardInterrupt\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "    image = Image.open(img_address).convert('RGB')\n",
      "    buf = self.recv_bytes()\n",
      "  File \"/home/razqtest1/anaconda2/lib/python2.7/site-packages/PIL/Image.py\", line 860, in convert\n",
      "    self.load()\n",
      "  File \"<ipython-input-14-e05f0342cee9>\", line 34, in __getitem__\n",
      "KeyboardInterrupt\n",
      "    image = self.transform(image)\n",
      "  File \"/home/razqtest1/anaconda2/lib/python2.7/site-packages/PIL/ImageFile.py\", line 234, in load\n",
      "  File \"/home/razqtest1/anaconda2/lib/python2.7/site-packages/torchvision-0.1.9-py2.7.egg/torchvision/transforms.py\", line 34, in __call__\n",
      "    n, err_code = decoder.decode(b)\n",
      "KeyboardInterrupt\n",
      "    img = t(img)\n",
      "  File \"/home/razqtest1/anaconda2/lib/python2.7/site-packages/torchvision-0.1.9-py2.7.egg/torchvision/transforms.py\", line 156, in __call__\n",
      "    t.sub_(m).div_(s)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-07cb8672f0b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m result = train_model(network, criterion, optimizer, trainLoader,\n\u001b[0;32m----> 2\u001b[0;31m                            valLoader, n_epochs = 10, use_gpu = True)\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-20-15edcfb47516>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(network, criterion, optimizer, trainLoader, valLoader, n_epochs, use_gpu)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0;31m# logging information.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m             \u001b[0mcum_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m             \u001b[0mmax_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0mcorrect\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmax_labels\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "result = train_model(network, criterion, optimizer, trainLoader,\n",
    "                           valLoader, n_epochs = 10, use_gpu = True)\n",
    "\n",
    "print result\n",
    "\n",
    "utils.plot_loss(result[2],result[3])\n",
    "utils.plot_accuracy(result[0],result[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
